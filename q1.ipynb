{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4a28c939dd2131a33d2c8b68c118b4d",
     "grade": false,
     "grade_id": "cell-6c9f551ea02c7bf9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img align=\"center\" src=\"figures/course.png\" width=\"800\">\n",
    "\n",
    "#                                    16720 (B) Neural Networks for Recognition - Assignment 3\n",
    "\n",
    "     Instructor: Kris Kitani                       TAs: Arka, Jinkun, Rawal, Rohan, Sheng-Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3b18c2962769d4c7f5fa2046057f9f2",
     "grade": false,
     "grade_id": "cell-372b7e8bfa64e2d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1 Implementing a Fully Connected Network (75 points)\n",
    "\n",
    "**Please include all the answers to the write-up questions to HW3:PDF**. Questions are indicated either the \"write-up\" or \"auto-grader\" tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10a52a255fcde49ea866eed7aab3f8fd",
     "grade": false,
     "grade_id": "cell-2d56327246e23156",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do Not Modify\n",
    "# Do Not Import ANY other packages\n",
    "import numpy as np\n",
    "\n",
    "# use for a \"no activation\" layer\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_deriv(post_act):\n",
    "    return np.ones_like(post_act)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(post_act):\n",
    "    return 1-post_act**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79ff1a768bc92c35caeaa705c9304131",
     "grade": false,
     "grade_id": "cell-cf0ebdeb56cae121",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.1 Network Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12e71c9c0c03aee4fd5317e95475fd79",
     "grade": false,
     "grade_id": "cell-810587ffe33a3598",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.1.1 (3 points, write-up)\n",
    "Why is it not a good idea to initialize a network with all zeros? If you imagine that every layer has weights and biases, what can a zero-initialized network output after training?\n",
    "\n",
    "<font color=\"red\">**Please include your answer to HW3:PDF**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "927e81c1fd404da2ff3d44df4620daac",
     "grade": false,
     "grade_id": "cell-72609a649db69760",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.1.2 (3 points, auto-grader)\n",
    "Implement `initialize_weights` below to initialize neural network weights with [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), where $Var[w] = \\frac{2}{n_{in}+ n_{out}} $ and $n$ is the dimensionality of the vectors. Please use an **uniform distribution** to sample random numbers (see eq 16 in [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)), we recommend using np.random.uniform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58275718a53d1e6bd9930a48cf0587bb",
     "grade": false,
     "grade_id": "cell-04dbce39e616d009",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_weights(in_size: int, out_size: int, params: dict, name: str='' ):\n",
    "    '''\n",
    "    Initialize the weights W and b for a linear layer Y = XW + b\n",
    "    \n",
    "    [input]\n",
    "    * in_size -- the feature dimension of the input\n",
    "    * out_size -- the feature dimension of the output\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    \n",
    "    HINTS:\n",
    "    (1) b should be a 1D array, not a 2D array with a singleton dimension\n",
    "    '''\n",
    "    W, b = None, None\n",
    "    \n",
    "    ## compute W and b using in_size and out_size.\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    W = np.random.uniform(-np.sqrt(6/float(in_size+out_size)), np.sqrt(6/float(in_size+out_size)), size=(in_size, out_size))\n",
    "    # b = np.random.uniform(-np.sqrt(6/float(in_size+out_size)), np.sqrt(6/float(in_size+out_size)), size=(out_size))\n",
    "    b = np.zeros((out_size))\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    params['W' + name] = W\n",
    "    params['b' + name] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "629cf17bed61b21fc736561554da254d",
     "grade": true,
     "grade_id": "q1_1_2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "initialize_weights(2, 25, params, 'layer1')\n",
    "initialize_weights(25, 4, params, 'output')\n",
    "assert(params['Wlayer1'].shape == (2, 25))\n",
    "assert(params['blayer1'].shape == (25,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a434d088469e76516c98458de89f8bf9",
     "grade": false,
     "grade_id": "cell-50fa916b36662a34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.1.3 (2 points, write-up)\n",
    "Why is it a good practice to initialize the parameters using random numbers? Explain the intuition behind scaling the initializations depending on layer size (see near Fig 6 in [Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))?\n",
    "\n",
    "<font color=\"red\">**Please include your answer to HW3:PDF**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbbf3ef2329ab0ab1f85d2a42bb8ea71",
     "grade": false,
     "grade_id": "cell-e1c75223ce84f014",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.2 Forward Propagation\n",
    "\n",
    "Please refer to `appendix.jpynb` for the forward propagation equations. We will be implementing the forward propagation in code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3aa4fd84feb6b3246703f07a9aa69e77",
     "grade": false,
     "grade_id": "cell-edc97acfe64d2a32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.1 (12 points, auto-grader)\n",
    "Implement `sigmoid`, along with `forward` propagation for a single layer with an activation function, namely\n",
    "$y = \\sigma(X W + b)$, returning the output and intermediate results for an $N \\times D$ dimension input $X$, with examples along the rows, data dimensions along the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80e796be52b7c3367508fb5e5fa498c0",
     "grade": false,
     "grade_id": "cell-9d629c29b34231c5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(X: np.ndarray):\n",
    "    '''\n",
    "    A sigmoid activation function\n",
    "    \n",
    "    [input]\n",
    "    * X -- input data [N x D]\n",
    "    \n",
    "    [output]\n",
    "    * res -- output after the sigmoid function\n",
    "    '''\n",
    "    res = None\n",
    "    \n",
    "    ## compute res using X\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    res = 1/(1+np.exp(-X))\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "258a357cb562b9b6497f4f45479673da",
     "grade": true,
     "grade_id": "q1_2_1_sigmoid",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test = sigmoid(np.array([-100,100]))\n",
    "assert test.min() < 1e-3\n",
    "assert test.max() > 1 - 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "057981ca81468558e373b7b99d89e4ef",
     "grade": false,
     "grade_id": "cell-8a7b5e6897a9d712",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def forward(X: np.ndarray, params: dict, name: str='',\n",
    "            activation: callable=sigmoid):\n",
    "    \"\"\"\n",
    "    Do a forward pass\n",
    "\n",
    "    [input]\n",
    "    * X -- input data [N x D]\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    * activation -- the activation function (default is sigmoid)\n",
    "    \n",
    "    [output]\n",
    "    * post_act -- output after a linear layer and activation\n",
    "    \"\"\"\n",
    "    pre_act, post_act = None, None\n",
    "    # get the layer parameters\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    \n",
    "    ## compute pre_act using X, W and b.\n",
    "    ## compute post_act using pre_act.\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pre_act = np.dot(X,W) + b\n",
    "    post_act = activation(pre_act)\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    # store the pre-activation and post-activation values\n",
    "    # these will be important in backprop\n",
    "    params['cache_' + name] = (X, pre_act, post_act)\n",
    "\n",
    "    return post_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8270a786b76230cf0608a396401307d4",
     "grade": true,
     "grade_id": "q1_2_1_forward",
     "locked": true,
     "points": 9,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "params = {'Wlayer1': np.random.rand(10, 25), 'blayer1': np.random.rand(25,)}\n",
    "X = np.random.rand(3, 10)\n",
    "y = forward(X, params, 'layer1')\n",
    "assert 'cache_layer1' in params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1efb13c6b9d712219e8635c09b571a7",
     "grade": false,
     "grade_id": "cell-e82fe9a6f4b520d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.2 (5 points, auto-grader)\n",
    "Implement the `softmax` function. Please implement a numerically stable computation of softmax using Theory:Q2. Hint, translate the input using the maximum element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4545cdaffdf5e1a0488899cf65ee514a",
     "grade": false,
     "grade_id": "cell-e41cfc344ff8bdef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(X: np.ndarray):\n",
    "    \"\"\"\n",
    "    A softmax function.\n",
    "    \n",
    "    [input]\n",
    "    * X -- input data [N x D]\n",
    "    \n",
    "    [output]\n",
    "    * res -- values after softmax\n",
    "    \"\"\"\n",
    "    res = None\n",
    "    \n",
    "    ## compute res using X\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    c = -np.max(X)\n",
    "    res = (np.exp(X+c)).transpose()/np.sum(np.exp(X+c), axis=1)\n",
    "    res = res.transpose()\n",
    "\n",
    "    # res = np.exp(X+c)/np.sum(np.exp(X+c), axis=0)\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ab6c7d80f21f9c99cf46dbc134250ae",
     "grade": true,
     "grade_id": "q1_2_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "df923e1e58fe4b4f853cf175ac6612f9",
     "grade": false,
     "grade_id": "cell-ca7affc876fccf2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.2.3 (5 points, auto-grader)\n",
    "Implement `compute_loss_and_acc` to compute the accuracy of a set of labels, along with the scalar loss across the data.  The loss function generally used for classification is the cross-entropy loss.\n",
    "\n",
    "$$L_{\\textbf{f}}(\\textbf{D}) = - \\sum_{(\\textbf{x}, \\textbf{y})\\in \\textbf{D}}\\textbf{y}\\cdot\\log(\\textbf{f}(\\textbf{x}))$$\n",
    "Here $\\textbf{D}$ is the full training dataset of data samples $\\textbf{x}$ ($N\\times 1$ vectors, N = dimensionality of data) and labels $\\textbf{y}$ ($C\\times 1$ one-hot vectors, C = number of classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d377251adf8aa34e97eafce05f017cf5",
     "grade": false,
     "grade_id": "cell-e8d3b0354b03c98c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss_and_acc(y: np.ndarray, probs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute total loss and accuracy\n",
    "    \n",
    "    [input]\n",
    "    * y -- one hot labels [N x C]\n",
    "    * probs -- class probabities [N x C]\n",
    "    \n",
    "    [output]\n",
    "    * loss -- cross-entropy loss\n",
    "    * acc -- accuracy\n",
    "    \"\"\"\n",
    "    loss, acc = None, None\n",
    "    \n",
    "    ## compute loss as a function of probs and y\n",
    "    ## compute acc using probs and y\n",
    "    # YOUR CODE HERE\n",
    "    loss = -np.sum(y * np.log(probs))\n",
    "    \n",
    "    T_label = np.argmax(y, axis = 1)\n",
    "    P_label = np.argmax(probs, axis = 1)\n",
    "    count = 0\n",
    "    for i in range(len(T_label)):\n",
    "        if T_label[i] == P_label[i]:\n",
    "            count += 1\n",
    "    \n",
    "    acc = count/y.shape[0]\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d27e1c1d859714e4fc20557d557f7e74",
     "grade": true,
     "grade_id": "q1_2_3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "958acb8267ab41402ed6ed648f137383",
     "grade": false,
     "grade_id": "cell-32109cdbdb894d52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.3 Backwards Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56fbf5998a62df67acc2890a35e3280c",
     "grade": false,
     "grade_id": "cell-f50e629f2d59ad01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.3.1 (10 points, auto-grader)\n",
    "Compute back-propagation for a single layer, given the original weights, the appropriate intermediate results, and given gradient with respect to the loss. You should return the gradient with respect to $X$ so you can feed it into the next layer. As a sanity check, your gradients should be the same dimensions as the original objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "495f65a53090819a1c4ac3176b95c89c",
     "grade": false,
     "grade_id": "cell-05a2aee350a000f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_deriv(post_act: np.ndarray):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid.\n",
    "    \n",
    "    we give this to you because you proved it\n",
    "    it's a function of post_act\n",
    "    \"\"\"\n",
    "    res = post_act*(1.0-post_act)\n",
    "    return res\n",
    "\n",
    "\n",
    "def backwards(delta: np.ndarray, params: dict, name: str='',\n",
    "              activation_deriv: callable=sigmoid_deriv):\n",
    "    \"\"\"\n",
    "    Do a backwards pass\n",
    "\n",
    "    [input]\n",
    "    * delta -- errors to backprop\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    * activation_deriv -- the derivative of the activation_func\n",
    "    \n",
    "    [output]\n",
    "    * grad_X -- gradient w.r.t X\n",
    "    \"\"\"\n",
    "    grad_X, grad_W, grad_b = None, None, None\n",
    "    # everything you may need for this layer\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    \n",
    "    X, pre_act, post_act = params['cache_' + name]\n",
    "    # print(X.shape)\n",
    "    # do the derivative through activation first\n",
    "    # then compute the derivative W,b, and X\n",
    "    # YOUR CODE HERE\n",
    "    # print('pre actka shape = ', pre_act.shape)\n",
    "    # print('X ka shape = ', X.shape)\n",
    "    # print(activation_deriv(post_act).shape)\n",
    "    # print(grad_b.shape)\n",
    "    # print(W.shape)\n",
    "    # print(delta.shape)\n",
    "    # print(grad_W.shape)\n",
    "    # grad_X = activation_deriv(post_act)*W*delta\n",
    "    # grad_b = np.dot(activation_deriv(post_act), delta.transpose())\n",
    "    # grad_W = activation_deriv(post_act)*X*delta\n",
    "    \n",
    "    # act_dash = activation_deriv(post_act)\n",
    "    act_dash = activation_deriv(post_act)\n",
    "    # print(np.ones((act_dash.shape[0])))\n",
    "    grad_b = np.dot(np.ones((act_dash.shape[0])), (act_dash*delta))\n",
    "    grad_W = np.dot(X.transpose(), (delta * act_dash))\n",
    "    grad_X = np.dot((act_dash*delta), W.transpose())\n",
    "    \n",
    "    # # raise NotImplementedError()\n",
    "\n",
    "    # store the gradients\n",
    "    params['grad_W' + name] = grad_W\n",
    "    params['grad_b' + name] = grad_b\n",
    "    # np.save('kshitij_W.npy', grad_W)\n",
    "    # np.save('kshitij_X.npy', grad_X)\n",
    "    # np.save('kshitij_b.npy', grad_b)\n",
    "    return grad_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b35b71d3a6a6929ebbfa9fd128a7767",
     "grade": true,
     "grade_id": "q1_3_1",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# we use random values to test your implementation \n",
    "# independent of previous questions\n",
    "n, c1, c2 = 5, 40, 20 \n",
    "rng = np.random.RandomState(12345)\n",
    "delta = np.random.rand(n, c2)\n",
    "name = 'layer1'\n",
    "params = {\n",
    "    'W'+name: np.random.rand(c1, c2),\n",
    "    'b'+name: np.random.rand(c2),\n",
    "    'cache_'+name: (np.random.rand(n, c1), \n",
    "                     np.random.rand(n, c2), \n",
    "                     np.random.rand(n, c2))\n",
    "}\n",
    "print()\n",
    "grad = backwards(delta, params, name, tanh_deriv)\n",
    "\n",
    "assert 'grad_W' + name in params\n",
    "assert 'grad_b' + name in params\n",
    "\n",
    "assert params['grad_W'+name].shape == params['W'+name].shape\n",
    "assert params['grad_b'+name].shape == params['b'+name].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41d97d29c9a5d4f9c9df877c531a3838",
     "grade": false,
     "grade_id": "cell-4fbc1b9ed142d412",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.4 Convolutional Layer (10 points)\n",
    "\n",
    "For now we have worked with linear layer in fully-connected networks. In practice, convolutional layers are commonly used to extract image feature. You will implement the forward and backawad propagation for convolutional layer in this subsection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bea742d6edacc6c63678c9527f3f9012",
     "grade": false,
     "grade_id": "cell-56852c776e6811a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.4.1 (5 points, auto-grader)\n",
    "Similar to Q1.2.1, implement `conv_forward` for a single convolutional layer with zero paddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[ 0,  1,  2,  3],\n",
    "       [ 4,  5,  6,  7],\n",
    "       [ 8,  9, 10, 11]]])\n",
    "b,c = a.shape[0],a.shape[1]\n",
    "print(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be891bcc9486e7726997d016f0ddbe4d",
     "grade": false,
     "grade_id": "cell-5e3595068856df10",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_forward(X: np.ndarray, params: dict, name: str='',\n",
    "            stride: int=1, pad: int=0):\n",
    "    \"\"\"\n",
    "    Do a forward pass for a convolutional layer\n",
    "\n",
    "    [input]\n",
    "    * X -- input data [N x C x H x W]\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    * stride, pad -- convolution parameters\n",
    "    \n",
    "    [output]\n",
    "    * res -- output after a convolutional layer\n",
    "    \"\"\"\n",
    "    res = None\n",
    "    # get the layer parameters\n",
    "    w = params['W' + name] # Conv Filter weights [F x C x HH x WW]\n",
    "    b = params['b' + name] # Biases [F]\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    row, col = X.shape[2], X.shape[3]\n",
    "    \n",
    "    h_row, h_col = w.shape[2], w.shape[3]\n",
    "    height = 1+int(((row-h_row+(2*pad)))/stride)\n",
    "    width = 1+int(((col-h_col+(2*pad)))/stride)\n",
    "#     res = np.zeros((N,F,H,W))\n",
    "    res = np.zeros((X.shape[0],w.shape[0],height,width))\n",
    "    # print('res shape = ', res.shape)\n",
    "    \n",
    "    for img_no in range(X.shape[0]):\n",
    "        windows = []\n",
    "        curr = X[img_no,:,:,:]\n",
    "        X_pad = np.pad(curr, ((0,0),(pad,pad),(pad,pad)), mode='constant')\n",
    "        for i in range(height):\n",
    "                for j in range(width):\n",
    "                        window = X_pad[:,i*stride:(i*stride)+h_row, j*stride:(j*stride)+h_col]\n",
    "                        # print('window shape = ', window.shape)\n",
    "                        windows.append(window.reshape(-1, 1))\n",
    "\n",
    "        image_to_convolve = np.concatenate(windows, axis=1)\n",
    "        # print('img2conv shape = ',image_to_convolve.shape)\n",
    "        \n",
    "        for filt_no in range(w.shape[0]):\n",
    "                filt = w[filt_no,:,:,:]\n",
    "                filt = filt.reshape(-1,1)\n",
    "                # print('filt shape before tile = ', filt.shape)\n",
    "                filt_window = np.tile(filt, (1,image_to_convolve.shape[-1]))\n",
    "                # print('filt shape after tile = ', filt_window.shape)\n",
    "                curr_conv = image_to_convolve * filt_window\n",
    "                curr_conv = np.sum(curr_conv, axis=0)\n",
    "                curr_conv = np.reshape(curr_conv, (height, width))\n",
    "                curr_conv = curr_conv + b[filt_no]\n",
    "                res[img_no,filt_no,:,:] = curr_conv\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    # store the input and convolution parameters\n",
    "    # these will be important in backprop\n",
    "    params['cache_' + name] = (X, stride, pad)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3bdddcb79670ea17ad0c5e167ea7e30",
     "grade": true,
     "grade_id": "q1_4_1",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0141824738238694e-09\n"
     ]
    }
   ],
   "source": [
    "x_shape = np.array((2, 3, 4, 4))\n",
    "w_shape = np.array((3, 3, 4, 4))\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape), dtype=np.float64).reshape(*x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape), dtype=np.float64).reshape(*w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3, dtype=np.float64)\n",
    "\n",
    "params = {'WConv_layer1': w, 'bConv_layer1': b}\n",
    "y = conv_forward(np.array(x), params, 'Conv_layer1', stride=2, pad=1)\n",
    "assert 'cache_Conv_layer1' in params\n",
    "\n",
    "\n",
    "y_ref = np.array([[[[-0.08759809, -0.10987781],\n",
    "                              [-0.18387192, -0.2109216 ]],\n",
    "                             [[ 0.21027089,  0.21661097],\n",
    "                              [ 0.22847626,  0.23004637]],\n",
    "                             [[ 0.50813986,  0.54309974],\n",
    "                              [ 0.64082444,  0.67101435]]],\n",
    "                            [[[-0.98053589, -1.03143541],\n",
    "                              [-1.19128892, -1.24695841]],\n",
    "                             [[ 0.69108355,  0.66880383],\n",
    "                              [ 0.59480972,  0.56776003]],\n",
    "                             [[ 2.36270298,  2.36904306],\n",
    "                              [ 2.38090835,  2.38247847]]]], \n",
    "            )\n",
    "# print('y shape = ', y.shape)\n",
    "# print('y_ref shape = ', y_ref.shape)\n",
    "assert y.shape == y_ref.shape\n",
    "\n",
    "max_diff = np.max(np.abs((y_ref - y)))\n",
    "base = (np.abs(y_ref) + np.abs(y)).clip(np.finfo(float).eps).max()\n",
    "print(max_diff/base) # the difference should be less than 1e-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c33dfa5415a8379ded48c910896ebc02",
     "grade": false,
     "grade_id": "cell-984447a667304ac8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.4.2 (5 points, auto-grader)\n",
    "Implement `conv_backword` for a single convolutional layer with zero paddings.\n",
    "Compute back-propagation for a single convolutional layer, given the original weights, the cached input, and given gradient with respect to the loss. Similar to Q1.3.1, you should return the gradient with respect to $X$ so you can feed it into the next layer. As a sanity check, your gradients should be the same dimensions as the original objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "058aa9f5d5dd5f8d6d878e39b2ff6ad1",
     "grade": false,
     "grade_id": "cell-cc53a61e4f74a964",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def conv_backward(delta: np.ndarray, params: dict, name: str=''):\n",
    "    \"\"\"\n",
    "    Do a backwards pass for a convolutional layer\n",
    "\n",
    "    [input]\n",
    "    * delta -- errors to backprop\n",
    "    * params -- a dictionary containing parameters\n",
    "    * name -- name of the layer\n",
    "    \n",
    "    [output]\n",
    "    * grad_X -- gradient w.r.t X\n",
    "    \"\"\"\n",
    "    grad_X, grad_W, grad_b = None, None, None\n",
    "    # everything you may need for this layer\n",
    "    W = params['W' + name]\n",
    "    b = params['b' + name]\n",
    "    X, stride, pad = params['cache_' + name]\n",
    "\n",
    "    # print('X shape = ', X.shape)\n",
    "    # print('W shape = ', W.shape)\n",
    "    # print('b shape = ', b.shape)\n",
    "    # print('delta shape = ', delta.shape)\n",
    "    # X shape =  (5, 4, 16, 16)\n",
    "    # delta shape =  (5, 8, 8, 8)\n",
    "    # W shape =  (8, 4, 7, 7)\n",
    "    # b shape =  (8,)\n",
    "    \n",
    "    # compute the derivative W,b, and X\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    row, col = X.shape[2], X.shape[3]\n",
    "    h_row, h_col = W.shape[2], W.shape[3]\n",
    "    height = 1+int(((row-h_row+(2*pad)))/stride)\n",
    "    width = 1+int(((col-h_col+(2*pad)))/stride)\n",
    "    # print('height  = ', height)\n",
    "#     res = np.zeros((N,F,H,W))\n",
    "    grad_W = np.zeros(W.shape)\n",
    "    # print('grad_W ka shape = ', grad_W.shape)\n",
    "    # print('grad_W shape = ', grad_W.shape)\n",
    "    \n",
    "    ##################### Good #########################\n",
    "    # for img_no in range(X.shape[0]):\n",
    "    #     # windows = np.zeros((delta.shape[2], delta.shape[3]))\n",
    "    #     windows = []\n",
    "    #     curr = X[img_no,:,:,:]\n",
    "    #     X_pad = np.pad(curr, ((0,0),(pad,pad),(pad,pad)), mode='constant')\n",
    "    #     for filt_no in range(delta.shape[1]):\n",
    "    #         for i in range(height):\n",
    "    #                 for j in range(width):\n",
    "    #                         window = X_pad[:,i*stride:(i*stride)+h_row, j*stride:(j*stride)+h_col]\n",
    "    #                         temp = window * delta[img_no,filt_no,i,j]\n",
    "    #                         # print('temp shape = ', temp.shape)\n",
    "    #                         grad_W[filt_no,:,:,:] += temp\n",
    "    ##################### Good #########################\n",
    "    \n",
    "    # grad_W1 = np.zeros(W.shape)\n",
    "    ##################### Better #######################\n",
    "    for img_no in range(X.shape[0]):\n",
    "        # windows = np.zeros((delta.shape[2], delta.shape[3]))\n",
    "        windows = []\n",
    "        curr = X[img_no,:,:,:]\n",
    "        X_pad = np.pad(curr, ((0,0),(pad,pad),(pad,pad)), mode='constant')\n",
    "        # for filt_no in range(delta.shape[1]):\n",
    "        for i in range(height):\n",
    "                for j in range(width):\n",
    "                        window = X_pad[:,i*stride:(i*stride)+h_row, j*stride:(j*stride)+h_col]\n",
    "                        # temp = window * delta[img_no,filt_no,i,j]\n",
    "                        # print('temp shape = ', temp.shape)\n",
    "                        # grad_W[filt_no,:,:,:] += temp\n",
    "                            \n",
    "                        # print('window shape = ', window.shape)\n",
    "                        windows.append(window.reshape(-1, 1))\n",
    "                        \n",
    "                        # windows.append(window)\n",
    "                        # print('windows ka size = ', len(windows))\n",
    "        # windows = np.array(windows)\n",
    "        # print('windows array shape = ', windows.shape)\n",
    "        # image_to_convolve = np.concatenate(windows, axis=1)\n",
    "        # print('img2conv shape = ',image_to_convolve.shape)\n",
    "        \n",
    "        for filt_no in range(delta.shape[1]):\n",
    "                filt = delta[img_no,filt_no,:,:]\n",
    "                filt = filt.reshape(-1,1)\n",
    "                # print('filt shape before tile = ', filt.shape)\n",
    "                filt_window = np.tile(filt, (1,window.shape[0]*window.shape[1]*window.shape[2]))\n",
    "                # filt_window = np.tile(filt, 10)\n",
    "                # print('filt shape after tile = ', filt_window.shape)\n",
    "                filt_window = filt_window[:,:,np.newaxis]\n",
    "                curr_conv = np.multiply(windows, filt_window)\n",
    "                # print('curr conv shape 1 = ', curr_conv.shape) \n",
    "                curr_conv = np.sum(curr_conv, axis=0)\n",
    "                # print('curr conv shape 2 = ', curr_conv.shape)\n",
    "                curr_conv = np.reshape(curr_conv, (W.shape[1],height-1, width-1))\n",
    "                # curr_conv = curr_conv + b[filt_no]\n",
    "                grad_W[filt_no,:,:,:] += curr_conv\n",
    "        # print('grad_W shape = ', grad_W.shape)\n",
    "    ##################### Better #######################\n",
    "\n",
    "    grad_X = np.zeros(X.shape)\n",
    "    grad_X_pad = np.pad(grad_X, ((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "    for N in range(delta.shape[0]):\n",
    "        del_layer = delta[N,:,:,:]\n",
    "        for i, j in zip(range(delta.shape[1]), range(W.shape[0])):\n",
    "            for p in range(delta.shape[2]):\n",
    "                for q in range(delta.shape[3]):\n",
    "                    X_window =  W[j,:,:,:] * delta[N,i,p,q]\n",
    "                    # print('X window shape = ', X_window.shape)\n",
    "                    grad_X_pad[N,:, p*stride:(p*stride)+W.shape[2], q*stride:(q*stride)+W.shape[3]] += X_window\n",
    "    grad_X = grad_X_pad[:,:,pad:grad_X_pad.shape[2]-pad,pad:grad_X_pad.shape[3]-pad]\n",
    "\n",
    "    grad_b = np.sum(delta, axis=(0,2,3))\n",
    "    # raise NotImplementedError()\n",
    "    # print('difference = ', np.sum(grad_W - grad_W1))\n",
    "    # store the gradients\n",
    "    params['grad_W' + name] = grad_W\n",
    "    params['grad_b' + name] = grad_b\n",
    "    return grad_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f1df3bc75ca1df7a721d8b116df16bb",
     "grade": true,
     "grade_id": "q1_4_2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(5, 4, 16, 16)\n",
    "w = np.random.rand(8, 4, 7, 7)\n",
    "b = np.random.rand(8,)\n",
    "dout = np.random.rand(5, 8, 8, 8)\n",
    "\n",
    "params = {'WConv_layer1': w, 'bConv_layer1': b}\n",
    "y = conv_forward(x, params, 'Conv_layer1', stride=2, pad=3)\n",
    "dx = conv_backward(dout, params, 'Conv_layer1')\n",
    "assert x.shape == dx.shape\n",
    "assert params['grad_WConv_layer1'].shape == params['WConv_layer1'].shape\n",
    "assert params['grad_bConv_layer1'].shape == params['bConv_layer1'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1ec406ff158fc34e56353b9584f4719",
     "grade": false,
     "grade_id": "cell-1ca2e358a302f02f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.5 The Training Loop\n",
    "You usually see gradient descent in three forms: \"normal\", \"stochastic\" and \"batch\". \"Normal\" gradient descent aggregates the updates for the entire dataset before changing the weights. Stochastic gradient descent applies updates after every single data example. Batch gradient descent is a compromise, where random subsets of the full dataset are evaluated before applying the gradient update. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e510393e089402c2b32c5d81eb2bc33",
     "grade": false,
     "grade_id": "cell-683134f646db0287",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.5.1 (10 points, auto-grader)\n",
    "Write a training loop that generates random batches, iterates over them for many iterations, does forward and backward propagation, and applies a gradient update step. Specifically, implement `get_random_batches` and `train` functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f1b9f89609b51466428af088518dc94",
     "grade": false,
     "grade_id": "cell-1b13b59b8a93b801",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_random_batches(x: np.ndarray, y: np.ndarray, batch_size: int) -> list:\n",
    "    import math\n",
    "    \"\"\"\n",
    "    Split x and y into random batches\n",
    "    \n",
    "    [input]\n",
    "    * x -- training samples\n",
    "    * y -- training lables\n",
    "    * batch_size -- batch size\n",
    "    \n",
    "    [output]\n",
    "    * batches -- a list of [(batch1_x,batch1_y)...]\n",
    "    \"\"\"\n",
    "    # \n",
    "    # return \n",
    "    batches = []\n",
    "    # YOUR CODE HERE\n",
    "    random_x = np.zeros(x.shape)\n",
    "    random_y = np.zeros(y.shape)\n",
    "    shuffeled = np.random.permutation(len(x))\n",
    "    # for old_idx, new_idx in enumerate(shuffeled):\n",
    "    #     random_x[new_idx] = x[old_idx]\n",
    "    #     random_y[new_idx] = y[old_idx]\n",
    "    random_x = x[shuffeled]\n",
    "    random_y = y[shuffeled]\n",
    "    \n",
    "    no_of_batches = math.ceil(x.shape[0]/batch_size)\n",
    "    for batchno in range(no_of_batches):\n",
    "        batches.append((random_x[batchno*batch_size:(batchno+1)*batch_size], random_y[batchno*batch_size:(batchno+1)*batch_size]))\n",
    "\n",
    "    # raise NotImplementedError()\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "446651dc1f57701ff98d71c44fe3d7f4",
     "grade": true,
     "grade_id": "q1_5_1_random_batches",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n, c1, c2 = 20, 100, 5\n",
    "# n, c1, c2 = 2, 2, 2\n",
    "batch_size = 3\n",
    "x = np.random.rand(n, c1)\n",
    "y = np.random.rand(n, c2)\n",
    "batches = get_random_batches(x, y, batch_size)\n",
    "assert type(batches) == list\n",
    "assert len(batches) >= 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "536e781d5a4a0684f44501d32a2be83d",
     "grade": false,
     "grade_id": "cell-96bdeec247659603",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train(x: np.ndarray, y: np.ndarray, params: dict, batch_size: int = 5,\n",
    "          max_iters: int = 500, learning_rate: float=1e-3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train the network with two sequential layers: \n",
    "    (1) one layer named \"layer1\" with sigmoid activation\n",
    "    (2) one layer named \"output\" with softmax activation\n",
    "\n",
    "    [input]\n",
    "    * x -- training samples\n",
    "    * y -- training lables\n",
    "    * params -- a dictionary containing initial parameters\n",
    "    * batch_size -- batch size\n",
    "    * max_iters -- total number of iterations\n",
    "    * learning_rate -- learning rate\n",
    "    \n",
    "    [output]\n",
    "    * total_loss, avg_acc -- loss and accuracy for the last iteration\n",
    "    \"\"\"\n",
    "\n",
    "    batches = get_random_batches(x, y, batch_size)\n",
    "\n",
    "    for itr in range(max_iters):\n",
    "        total_loss = 0\n",
    "        avg_acc = 0\n",
    "        for xb, yb in batches:\n",
    "\n",
    "            # forward\n",
    "            # YOUR CODE HERE\n",
    "            post_act = forward(xb,params,'layer1',sigmoid)\n",
    "            pred_output = forward(post_act,params,'output',softmax)\n",
    "            # raise NotImplementedError()\n",
    "            \n",
    "            # loss\n",
    "            # be sure to add loss and accuracy to epoch totals\n",
    "            # YOUR CODE HERE\n",
    "            loss, acc = compute_loss_and_acc(yb, pred_output)\n",
    "            total_loss += loss\n",
    "            avg_acc += acc/len(batches)\n",
    "            # raise NotImplementedError()\n",
    "            \n",
    "            # backward\n",
    "            # YOUR CODE HERE\n",
    "            last_layer_backprop = backwards(pred_output - yb, params, 'output', linear_deriv)\n",
    "            hidden_layer_backprop  = backwards(last_layer_backprop, params, 'layer1', sigmoid_deriv)\n",
    "            # raise NotImplementedError()\n",
    "\n",
    "            # apply gradient\n",
    "            # YOUR CODE HERE\n",
    "            params['Woutput'] = params['Woutput'] - learning_rate*params['grad_Woutput']\n",
    "            params['boutput'] = params['boutput'] - learning_rate*params['grad_boutput']\n",
    "            params['Wlayer1'] = params['Wlayer1'] - learning_rate*params['grad_Wlayer1']\n",
    "            params['blayer1'] = params['blayer1'] - learning_rate*params['grad_blayer1']\n",
    "            # raise NotImplementedError()\n",
    "            \n",
    "        if itr % 100 == 0:\n",
    "            print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(\n",
    "                itr, total_loss, avg_acc))\n",
    "    return total_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b42a354df1f71bfacbf65947af28e7f0",
     "grade": true,
     "grade_id": "q1_5_1_train",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 00 \t loss: 59.82 \t acc : 0.25\n",
      "itr: 100 \t loss: 42.56 \t acc : 0.55\n",
      "itr: 200 \t loss: 35.67 \t acc : 0.62\n",
      "itr: 300 \t loss: 31.87 \t acc : 0.70\n",
      "itr: 400 \t loss: 29.75 \t acc : 0.72\n",
      "itr: 500 \t loss: 28.30 \t acc : 0.77\n"
     ]
    }
   ],
   "source": [
    "# Successulf implementation of dependent functions are required to get full score for the `train` function\n",
    "\n",
    "# create inputs\n",
    "g0 = np.random.multivariate_normal([3.6,40],[[0.05,0],[0,10]],10)\n",
    "g1 = np.random.multivariate_normal([3.9,10],[[0.01,0],[0,5]],10)\n",
    "g2 = np.random.multivariate_normal([3.4,30],[[0.25,0],[0,5]],10)\n",
    "g3 = np.random.multivariate_normal([2.0,10],[[0.5,0],[0,10]],10)\n",
    "x = np.vstack([g0,g1,g2,g3])\n",
    "\n",
    "# create labels\n",
    "y_idx = np.array([0 for _ in range(10)] + [1 for _ in range(10)] + [2 for _ in range(10)] + [3 for _ in range(10)])\n",
    "\n",
    "# turn labels to one_hot\n",
    "y = np.zeros((y_idx.shape[0],y_idx.max()+1))\n",
    "y[np.arange(y_idx.shape[0]),y_idx] = 1\n",
    "\n",
    "# parameters in a dictionary\n",
    "params = {}\n",
    "# initialize a layer\n",
    "initialize_weights(2,25,params,'layer1')\n",
    "initialize_weights(25,4,params,'output')\n",
    "\n",
    "# train the two-layer neural network\n",
    "total_loss, avg_acc = train(x, y, params, batch_size=5, max_iters=500, learning_rate=1e-3)\n",
    "print(\"itr: {:02d} \\t loss: {:.2f} \\t acc : {:.2f}\".format(500, total_loss, avg_acc))\n",
    "\n",
    "# with default settings, you should get loss < 35 and accuracy > 70%\n",
    "assert total_loss < 35 and avg_acc > 0.70\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8899e5620b778603871026cee269bbc",
     "grade": false,
     "grade_id": "cell-912ef865c23093b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.6 Numerical Gradient Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5580c4ba31d85d2e2c246aa0e64ed48d",
     "grade": false,
     "grade_id": "cell-da874ad741d95189",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q1.6.1 (15 points, auto-grader)\n",
    "Implement the `centeral_differences_gradient` function. Instead of using the analytical gradients computed from the chain rule, add $\\epsilon$ offset to each element in the weights, and compute the numerical gradient of the loss with central differences. Central differences is just $\\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2 \\epsilon}$. Remember, this needs to be done for each scalar dimension in all of your weights independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5533bfa73539c91977ba16fb704b7bb7",
     "grade": false,
     "grade_id": "cell-2e7c04b3366231a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def centeral_differences_gradient(params: dict, eps = 1e-6):\n",
    "    \"\"\"\n",
    "    Compute the estimated gradients using central difference\n",
    "    \n",
    "    Hint:\n",
    "    please feel free to reuse the functions above\n",
    "    \"\"\"\n",
    "    for k, v in params.items():\n",
    "        if '_' in k:\n",
    "            continue\n",
    "        # we have a real parameter!\n",
    "        # for each value inside the parameter\n",
    "        #   subtract epsilon\n",
    "        #   run the forward function by \n",
    "        #   get the loss\n",
    "        #   add epsilon\n",
    "        #   run the forward function\n",
    "        #   get the loss\n",
    "        #   compute derivative with central diffs\n",
    "    \n",
    "        # YOUR CODE HERE\n",
    "        if k == 'Woutput' or k == 'Wlayer1':\n",
    "            for i in range(v.shape[0]):\n",
    "                for j in range(v.shape[1]):\n",
    "                    v[i][j] = v[i][j] - eps\n",
    "                    fwd_prop11 = forward(x,params,'layer1')\n",
    "                    op1 = forward(fwd_prop11, params, 'output', softmax)\n",
    "                    loss1, acc1 = compute_loss_and_acc(y, op1)\n",
    "                    v[i][j] = v[i][j] + 2*eps\n",
    "                    fwd_prop21 = forward(x,params,'layer1')\n",
    "                    op2 = forward(fwd_prop21, params, 'output', softmax)\n",
    "                    loss2, acc2 = compute_loss_and_acc(y, op2)\n",
    "                    params[\"grad_\"+k][i][j] = (loss2 - loss1)/(2*eps)\n",
    "                    v[i][j] = v[i][j] - eps\n",
    "        elif k == 'boutput' or k == 'blayer1':\n",
    "            for i in range(v.shape[0]):\n",
    "                v[i] = v[i] - eps\n",
    "                fwd_prop11 = forward(x,params,'layer1')\n",
    "                op1 = forward(fwd_prop11, params, 'output', softmax)\n",
    "                loss1, acc1 = compute_loss_and_acc(y, op1)\n",
    "                v[i] = v[i] + 2*eps\n",
    "                fwd_prop21 = forward(x,params,'layer1')\n",
    "                op2 = forward(fwd_prop21, params, 'output', softmax)\n",
    "                loss2, acc2 = compute_loss_and_acc(y, op2)\n",
    "                params['grad_'+k][i] = (loss2 - loss1)/(2*eps)                \n",
    "                v[i] = v[i] - eps\n",
    "        # raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f38f50df99b3b1d8c1d3074a5a2b021",
     "grade": true,
     "grade_id": "q1_6_1",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erroe =  7.766246036131394e-06\n",
      "Test case passed, good job!\n"
     ]
    }
   ],
   "source": [
    "# Compute the analytical gradients\n",
    "h1 = forward(x,params,'layer1')\n",
    "probs = forward(h1,params,'output',softmax)\n",
    "delta1 = probs\n",
    "delta1[np.arange(probs.shape[0]),y_idx] -= 1\n",
    "\n",
    "delta2 = backwards(delta1,params,'output',linear_deriv)\n",
    "backwards(delta2,params,'layer1',sigmoid_deriv)\n",
    "\n",
    "import copy\n",
    "params_orig = copy.deepcopy(params)\n",
    "\n",
    "# Compute the estimated gradient using central difference\n",
    "centeral_differences_gradient(params)\n",
    "\n",
    "total_error = 0\n",
    "for k in params.keys():\n",
    "    if 'grad_' in k:\n",
    "        # relative error\n",
    "        err = np.abs(params[k] - params_orig[k])/np.maximum(np.abs(params[k]),np.abs(params_orig[k]))\n",
    "        err = err.sum()\n",
    "        total_error += err\n",
    "# should be less than 1e-4\n",
    "print('erroe = ', total_error)\n",
    "assert 0. < total_error < 1e-4\n",
    "print('Test case passed, good job!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
